{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Your Title\"\n",
        "format: \n",
        "  pdf:\n",
        "    keep-tex: true\n",
        "    include-in-header: \n",
        "       text: |\n",
        "         \\usepackage{fvextra}\n",
        "         \\DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\\\\{\\}}\n",
        "include-before-body:\n",
        "  text: |\n",
        "    \\RecustomVerbatimEnvironment{verbatim}{Verbatim}{\n",
        "      showspaces = false,\n",
        "      showtabs = false,\n",
        "      breaksymbolleft={},\n",
        "      breaklines\n",
        "    }\n",
        "---\n",
        "\n",
        "\n",
        "**PS4:** Due Sat Nov 2 at 5:00PM Central. Worth 100 points. \n",
        "We use (`*`) to indicate a problem that we think might be time consuming. \n",
        "    \n",
        "## Style Points (10 pts) \n",
        "Please refer to the minilesson on code style\n",
        "**[here](https://uchicago.zoom.us/rec/share/pG_wQ-pHTQrJTmqNn4rcrw5V194M2H2s-2jdy8oVhWHkd_yZt9o162IWurpA-fxU.BIQlSgZLRYctvzp-)**.\n",
        "\n",
        "\n",
        "## Submission Steps (10 pts)\n",
        "1. This problem set is a paired problem set.\n",
        "2. Play paper, scissors, rock to determine who goes first. Call that person *Partner 1*.\n",
        "    - Partner 1 (name and cnet ID): Alejandra Silva - aosilva\n",
        "    - Partner 2 (name and cnet ID): Guillermina Marto - gmarto\n",
        "3. Partner 1 will accept the `ps4` and then share the link it creates with their partner. You can only share it with one partner so you will not be able to change it after your partner has accepted. \n",
        "4. \"This submission is our work alone and complies with the 30538 integrity policy.\" Add your initials to indicate your agreement: **AS** **GM**\n",
        "5. \"I have uploaded the names of anyone else other than my partner and I worked with on the problem set **[here](https://docs.google.com/forms/d/185usrCREQaUbvAXpWhChkjghdGgmAZXA3lPWpXLLsts/edit)**\"  (1 point)\n",
        "6. Late coins used this pset: \\*\\*\\_\\_\\*\\* Late coins left after submission: \\*\\*\\_\\_\\*\\*\n",
        "7. Knit your `ps4.qmd` to an PDF file to make `ps4.pdf`, \n",
        "    * The PDF should not be more than 25 pages. Use `head()` and re-size figures when appropriate. \n",
        "8. (Partner 1): push  `ps4.qmd` and `ps4.pdf` to your github repo.\n",
        "9. (Partner 1): submit `ps4.pdf` via Gradescope. Add your partner on Gradescope.\n",
        "10. (Partner 1): tag your submission in Gradescope\n",
        "\n",
        "**Important:** Repositories are for tracking code. **Do not commit the data or shapefiles to your repo.** The best way to do this is with `.gitignore`, which we have covered in class. If you do accidentally commit the data, Github has a [guide](https://docs.github.com/en/repositories/working-with-files/managing-large-files/about-large-files-on-github#removing-files-from-a-repositorys-history). The best course of action depends on whether you have pushed yet. This also means that both partners will have to download the initial raw data and any data cleaning code will need to be re-run on both partners' computers. \n",
        "\n",
        "\n",
        "## Download and explore the Provider of Services (POS) file (10 pts)\n",
        "\n",
        "1. \n"
      ],
      "id": "c7e21cfa"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import altair as alt\n",
        "\n",
        "base_url = \"https://data.cms.gov/data-api/v1/dataset/{uuid}/data\"\n",
        "uuid = \"96ba2257-2080-49c1-9e5b-7726f9f83cad\"\n",
        "\n",
        "columns = [\n",
        "    \"PRVDR_CTGRY_CD\",        # Provider Category Code\n",
        "    \"PRVDR_CTGRY_SBTYP_CD\",  # Provider Subtype Code\n",
        "    \"PRVDR_NUM\",             # CMS Certification Number\n",
        "    \"PGM_TRMNTN_CD\",         # Termination Code\n",
        "    \"FAC_NAME\",              # Facility Name\n",
        "    \"ZIP_CD\",                # ZIP Code\n",
        "    \"STATE_CD\"               # State Abbreviation\n",
        "]\n",
        "\n",
        "columns_param = \",\".join(columns)\n",
        "\n",
        "offset = 0\n",
        "limit = 5000  #  API allows size to be set to 5000\n",
        "\n",
        "all_data = []\n",
        "\n",
        "while True:\n",
        "    params = {\n",
        "\n",
        "        \"column\": columns_param,\n",
        "        \"size\": limit,\n",
        "        \"offset\": offset\n",
        "    }\n",
        "\n",
        "    url = base_url.format(uuid=uuid)\n",
        "    response = requests.get(url, params=params)\n",
        "\n",
        "\n",
        "\n",
        "    data = response.json()\n",
        "\n",
        " \n",
        "    all_data.extend(data)\n",
        "\n",
        "    offset += limit\n",
        "\n",
        "df = pd.DataFrame(all_data)\n",
        "\n",
        "\n",
        "df.to_csv(\"pos2016.csv\", index=False)\n"
      ],
      "id": "776a769d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. "
      ],
      "id": "cc292fbe"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df = pd.read_csv(\"pos2016.csv\")\n",
        "\n",
        "\n",
        "df_st_hospitals = df[\n",
        "    (df[\"PRVDR_CTGRY_CD\"] == 1) & \n",
        "    (df[\"PRVDR_CTGRY_SBTYP_CD\"] == 1)\n",
        "]\n",
        "\n",
        "num_hospitals = df_st_hospitals.shape[0]\n",
        "print(f\"Number of short-term hospitals reported in the data: {num_hospitals}\")\n",
        "\n",
        "print(df_st_hospitals)"
      ],
      "id": "f9696925",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The number of short-term hospitals reported in the dataset for Q4 2016 is 7,245.\n",
        "\n",
        "According to the American Hospital Association (AHA) Annual Survey, the estimated number of short-term hospitals is 4,500–5,000. Similarly, the CMS Hospital Compare dataset indicates around 4,800 hospitals.\n",
        "\n",
        "The discrepancy could be due to the narrower definition used in our dataset and the timing of data collection, which only includes hospitals in Q4 2016. Additionally, the CMS dataset might not include hospitals that do not participate in Medicare or Medicaid, which could lead to lower numbers.\n",
        "\n",
        "3. \n"
      ],
      "id": "25e0c28b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "uuid_dict = {\n",
        "    \"2016Q4\": \"96ba2257-2080-49c1-9e5b-7726f9f83cad\",\n",
        "    \"2017Q4\": \"d338dc0d-641c-486a-b586-88a662f36963\",\n",
        "    \"2018Q4\": \"4ff7fcfb-2a40-4f76-875d-a4ac2aec268e\",\n",
        "    \"2019Q4\": \"03cca0cc-13a0-4b8d-82c4-57185b6bbfbd\"\n",
        "}\n",
        "\n",
        "columns = [\n",
        "    \"PRVDR_CTGRY_CD\",        # Provider Category Code\n",
        "    \"PRVDR_CTGRY_SBTYP_CD\",  # Provider Subtype Code\n",
        "    \"PRVDR_NUM\",             # CMS Certification Number\n",
        "    \"PGM_TRMNTN_CD\",         # Termination Code\n",
        "    \"FAC_NAME\",              # Facility Name\n",
        "    \"ZIP_CD\",                # ZIP Code\n",
        "    \"STATE_CD\"               # State Abbreviation\n",
        "]\n",
        "\n",
        "columns_param = \",\".join(columns)\n",
        "\n",
        "combined_data = []\n",
        "\n",
        "for year_quarter, uuid in uuid_dict.items():\n",
        "    offset = 0\n",
        "    limit = 5000  \n",
        "    all_data = []\n",
        "\n",
        "    print(f\"Fetching data for {year_quarter}...\")\n",
        "\n",
        "    while True:\n",
        "        params = {\n",
        "            \"column\": columns_param,\n",
        "            \"size\": limit,\n",
        "            \"offset\": offset\n",
        "        }\n",
        "\n",
        "        url = f\"https://data.cms.gov/data-api/v1/dataset/{uuid}/data\"\n",
        "        response = requests.get(url, params=params)\n",
        "\n",
        "        if response.status_code != 200:\n",
        "            print(f\"Error: {response.status_code}, {response.text}\")\n",
        "            break\n",
        "\n",
        "        data = response.json()\n",
        "\n",
        "        if not data:\n",
        "            print(\"No more data available.\")\n",
        "            break\n",
        "\n",
        "        all_data.extend(data)\n",
        "\n",
        "        offset += limit\n",
        "        print(f\"Fetched {len(data)} rows for {year_quarter}, moving to next batch...\")\n",
        "\n",
        "    year_data = pd.DataFrame(all_data)\n",
        "    year_data[\"Year\"] = year_quarter[:4]\n",
        "\n",
        "    # filtro por las condiciones\n",
        "    year_data = year_data[\n",
        "    (year_data[\"PRVDR_CTGRY_CD\"] == \"01\") & \n",
        "    (year_data[\"PRVDR_CTGRY_SBTYP_CD\"] == \"01\")\n",
        "]  \n",
        "\n",
        "    combined_data.append(year_data)\n",
        "\n",
        "combined_df = pd.concat(combined_data, axis=0)\n",
        "\n",
        "combined_df.to_csv(\"combined_data.csv\", index=False)\n",
        "\n",
        "print(f\"Total records retrieved across all years: {combined_df.shape[0]}\")"
      ],
      "id": "9e0d2808",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import altair as alt\n",
        "\n",
        "# Plotting Number of Observations Per Year\n",
        "\n",
        "combined_year_df = combined_df.groupby(\"Year\").size().reset_index(name=\"Number of Observations\")\n",
        "\n",
        "obs_chart = alt.Chart(combined_year_df).mark_bar().encode(\n",
        "    x=alt.X(\"Year:O\", title=\"Year\"),  \n",
        "    y=alt.Y(\"Number of Observations:Q\", title=\"Number of Observations\"),\n",
        "    tooltip=[\"Year\", \"Number of Observations\"]\n",
        ").properties(\n",
        "    title=\"Number of Observations Per Year\"\n",
        ")\n",
        "\n",
        "obs_chart.display()\n",
        "\n",
        "\n",
        "# Plotting Number of Unique Hospitals Per Year\n",
        "unique_hospitals = combined_df.groupby(\"Year\")[\"PRVDR_NUM\"].nunique().reset_index(name=\"Number of Unique Hospitals\")\n",
        "\n",
        "unique_hospitals_chart = alt.Chart(unique_hospitals).mark_bar().encode(\n",
        "    x=alt.X(\"Year:O\", title=\"Year\"), \n",
        "    y=alt.Y(\"Number of Unique Hospitals:Q\", title=\"Number of Unique Hospitals\"),\n",
        "    tooltip=[\"Year\", \"Number of Unique Hospitals\"]\n",
        ").properties(\n",
        "    title=\"Number of Unique Hospitals Per Year\"\n",
        ")\n",
        "\n",
        "unique_hospitals_chart.display()\n",
        "\n",
        "\n",
        "#print(\"Observations Per Year:\")\n",
        "#print(observations_per_year)\n",
        "#print(\"\\nUnique Hospitals Per Year:\")\n",
        "#print(unique_hospitals_per_year)\n",
        "\n",
        "# Compare the two plots to understand the structure of the data.\n",
        "# Observations per year may be higher due to multiple records for the same hospital.\n",
        "# Unique hospitals per year give an idea of how many distinct hospitals are in the dataset for each year."
      ],
      "id": "2ade4ad4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4. \n",
        "    a.\n",
        "    b.\n",
        "\n",
        "## Identify hospital closures in POS file (15 pts) (*)\n",
        "\n",
        "1. Termination code equal to 00=ACTIVE PROVIDER. The data contain only up to the code 07. The other codes apply to CLIA. \n"
      ],
      "id": "5df7aa57"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "combined_df[\"PGM_TRMNTN_CD\"] = combined_df[\"PGM_TRMNTN_CD\"].astype(str)\n",
        "\n",
        "inactive_codes = [\"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\"]\n",
        "\n",
        "active_2016 = combined_df[(combined_df[\"Year\"] == \"2016\") & (combined_df[\"PGM_TRMNTN_CD\"] == \"00\")]\n",
        "\n",
        "print(active_2016.head())\n",
        "\n",
        "suspected_closures = []\n",
        "\n",
        "for idx, hospital in active_2016.iterrows():\n",
        "    provider_num = hospital[\"PRVDR_NUM\"]\n",
        "    facility_name = hospital[\"FAC_NAME\"]\n",
        "    zip_code = hospital[\"ZIP_CD\"]\n",
        "    \n",
        "    for year in [\"2017\", \"2018\", \"2019\"]:\n",
        "        yearly_data = combined_df[(combined_df[\"PRVDR_NUM\"] == provider_num) & (combined_df[\"Year\"] == year)]\n",
        "        \n",
        "        if yearly_data.empty or yearly_data[\"PGM_TRMNTN_CD\"].isin(inactive_codes).any():\n",
        "            suspected_closures.append({\n",
        "                \"Provider Number\": provider_num,\n",
        "                \"Facility Name\": facility_name,\n",
        "                \"ZIP Code\": zip_code,\n",
        "                \"Year Closed\": year\n",
        "            })\n",
        "            break  \n",
        "\n",
        "suspected_closures_df = pd.DataFrame(suspected_closures)\n",
        "num_closures = len(suspected_closures_df)\n",
        "\n",
        "display(f\"Total suspected hospital closures: {num_closures}\")\n",
        "display(suspected_closures_df.head()) \n"
      ],
      "id": "89bdf1f6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. \n"
      ],
      "id": "d6ee11e8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "sorted_closures = suspected_closures_df.sort_values(by=\"Facility Name\")\n",
        "\n",
        "top_10_closures = sorted_closures[[\"Facility Name\", \"Year Closed\"]].head(10)\n",
        "\n",
        "display(top_10_closures)"
      ],
      "id": "917582a6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3. \n"
      ],
      "id": "40b0d3c5"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Count the number of active hospitals (PGM_TRMNTN_CD == \"00\") by ZIP_CD (zip code) for each year\n",
        "active_hospitals_by_year = (\n",
        "    combined_df[combined_df['PGM_TRMNTN_CD'] == \"00\"]\n",
        "    .groupby(['ZIP_CD', 'Year'])\n",
        "    .size()\n",
        "    .unstack(fill_value=0)\n",
        ")\n",
        "\n",
        "# Step 2: Create columns with differences between years for each ZIP_CD\n",
        "# Calculate the differences between consecutive years\n",
        "active_hospitals_by_year['diff_2017_2016'] = active_hospitals_by_year[\"2017\"] - \\\n",
        "    active_hospitals_by_year[\"2016\"]\n",
        "active_hospitals_by_year['diff_2018_2017'] = active_hospitals_by_year[\"2018\"] - \\\n",
        "    active_hospitals_by_year[\"2017\"]\n",
        "active_hospitals_by_year['diff_2019_2018'] = active_hospitals_by_year[\"2019\"] - \\\n",
        "    active_hospitals_by_year[\"2018\"]\n",
        "\n",
        "# Reset index to prepare for merging\n",
        "active_hospitals_by_year = active_hospitals_by_year.reset_index()\n",
        "\n",
        "# Step 3: Merge the active_hospitals_by_year dataframe with suspected_closures by ZIP Code\n",
        "merged_df = pd.merge(suspected_closures_df, active_hospitals_by_year,\n",
        "                     left_on='ZIP Code', right_on='ZIP_CD', how='left')\n",
        "\n",
        "# Initialize the closure_difference column with NaN\n",
        "merged_df['closure_difference'] = np.nan\n",
        "\n",
        "# Loop through each row and set closure_difference based on Year Closed\n",
        "for index, row in merged_df.iterrows():\n",
        "    if row['Year Closed'] == \"2017\":\n",
        "        merged_df.at[index, 'closure_difference'] = row['diff_2018_2017']\n",
        "    elif row['Year Closed'] == \"2018\":\n",
        "        merged_df.at[index, 'closure_difference'] = row['diff_2019_2018']\n",
        "    else:\n",
        "        # Optionally set it to a specific value if Year Closed doesn't match any condition\n",
        "        merged_df.at[index, 'closure_difference'] = None\n",
        "\n",
        "# Step 5: Create a dummy variable that is 1 if the closure_difference is 0 or greater, else 0\n",
        "merged_df['closure_dummy'] = merged_df['closure_difference'].apply(\n",
        "    lambda x: 1 if pd.notna(x) and x > 0 else (0 if pd.notna(x) else np.nan)\n",
        ")\n",
        "\n",
        "# Count the number of rows where closure_dummy is 1\n",
        "count_of_ones = merged_df['closure_dummy'].sum()\n",
        "display(f\"Number of rows with closure_dummy = 1: {count_of_ones}\")\n",
        "\n",
        "# Count the number of rows where closure_dummy is 0\n",
        "count_of_zeros = (merged_df['closure_dummy'] == 0).sum()\n",
        "display(f\"Number of rows with closure_dummy = 0: {count_of_zeros}\")\n"
      ],
      "id": "0ea2fc7c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Question a: Count hospitals that fit the definition of potentially being a merger/acquisition\n",
        "count_of_ones = merged_df['closure_dummy'].sum()\n",
        "display(f\"Number of hospitals potentially being a merger/acquisition: {count_of_ones}\")\n",
        "\n",
        "# Question b: Count hospitals left after correcting for potential mergers\n",
        "corrected_closures = merged_df[merged_df['closure_dummy'] == 0]\n",
        "count_of_zeros = (merged_df['closure_dummy'] == 0).sum()\n",
        "display(f\"Number of hospitals left after correcting for potential mergers: {count_of_zeros}\")\n",
        "\n",
        "# Question c: Sort the corrected list of closures by hospital name and display the first 10 rows\n",
        "sorted_corrected_closures = corrected_closures.sort_values(by='Facility Name').head(10)\n",
        "display(sorted_corrected_closures[['Facility Name', 'ZIP Code', 'Year Closed', 'closure_difference']])"
      ],
      "id": "edb1c7e9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Download Census zip code shapefile (10 pt) \n",
        "\n",
        "1. \n",
        "    a. **Five File Types and Their Contents**\n",
        "\n",
        "Based on the files provided, here is a summary of the five common shapefile-related formats and their typical contents:\n",
        "\n",
        "1. **`.shp` (Shapefile)**: Stores the geometry of the feature, such as points, lines, or polygons representing geographic boundaries (e.g., ZIP code tabulation areas) (6.4 MB).\n",
        "2. **`.shx` (Shape Index)**: Contains an index of the feature geometry, providing quick access to spatial data in the `.shp` file (165 bytes).\n",
        "3. **`.dbf` (Database File)**: Stores attribute data associated with each shape, such as names, identifiers, and other relevant information (837.5 MB).\n",
        "4. **`.prj` (Projection File)**: Contains projection information, specifying the coordinate system and projection details for the spatial data (265 KB).\n",
        "5. **`.xml` (Metadata File)**: Provides metadata, including details about the data's origin, purpose, spatial extent, and additional descriptive information (16 KB).\n",
        "\n",
        "    b. **File Sizes After Unzipping**\n",
        "\n",
        "The sizes of the files were included above. However, to determine the size of each file after unzipping, use the following command in the terminal of the folder:\n",
        "\n",
        "\n",
        "total 1648944\n",
        "-rw-r--r--@ 1 aosilva  staff   6.1M Sep 14  2011 gz_2010_us_860_00_500k.dbf\n",
        "-rw-r--r--@ 1 aosilva  staff   165B Sep 14  2011 gz_2010_us_860_00_500k.prj\n",
        "-rw-r--r--@ 1 aosilva  staff   799M Sep 14  2011 gz_2010_us_860_00_500k.shp\n",
        "-rw-r--r--@ 1 aosilva  staff   259K Sep 14  2011 gz_2010_us_860_00_500k.shx\n",
        "-rw-rw-rw-@ 1 aosilva  staff    15K Dec  9  2011 gz_2010_us_860_00_500k\n",
        "\n",
        "\n",
        "2. \n"
      ],
      "id": "59f747bd"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import geopandas as gpd\n",
        "\n",
        "# Load the shapefile (replace 'path_to_shapefile' with the actual path)\n",
        "zip_shapefile = gpd.read_file('gz_2010_us_860_00_500k/gz_2010_us_860_00_500k.shp')\n",
        "\n",
        "# Filter for Texas ZIP codes (750-799)\n",
        "texas_zip_shapefile = zip_shapefile[zip_shapefile['ZCTA5'].str.startswith(tuple(map(str, range(750, 799))))]\n",
        "\n",
        "# Check the result\n",
        "texas_zip_shapefile.head()\n",
        "\n",
        "# Filter combined_df for 2016 and Texas ZIP codes\n",
        "hospitals_2016 = combined_df[(combined_df['Year'] == 2016) & \n",
        "                             (combined_df['ZIP_CD'].str.startswith(tuple(map(str, range(750, 799)))))]\n",
        "\n",
        "# Count hospitals per ZIP code\n",
        "hospitals_per_zip = hospitals_2016.groupby('ZIP_CD').size().reset_index(name='hospital_count')\n",
        "\n",
        "# Merge shapefile with hospital counts\n",
        "texas_zip_map = texas_zip_shapefile.merge(hospitals_per_zip, left_on='ZCTA5', right_on='ZIP_CD', how='left')\n",
        "\n",
        "# Fill NaN values with 0 for ZIP codes with no hospitals\n",
        "texas_zip_map['hospital_count'] = texas_zip_map['hospital_count'].fillna(0)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot the choropleth\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
        "texas_zip_map.plot(column='hospital_count', cmap='YlGnBu', linewidth=0.8, ax=ax, edgecolor='0.8', legend=True)\n",
        "ax.set_title('Number of Hospitals by ZIP Code in Texas (2016)')\n",
        "ax.axis('off')\n",
        "plt.show()\n"
      ],
      "id": "395dfb5c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Calculate zip code’s distance to the nearest hospital (20 pts) (*)\n",
        "\n",
        "1. \n",
        "\n",
        "The GeoDataFrame, `zips_all_centroids`, has dimensions that correspond to the number of unique ZIP code areas, with two main columns: `ZIP Code` and `geometry`. The `ZIP Code` column holds the specific ZIP Code Tabulation Area (ZCTA) for each entry, representing each distinct postal region. The `geometry` column contains the centroid point of each ZIP code area, with each point indicating the latitude and longitude coordinates for the center of the ZIP code region. This structure allows for further geographic analysis, such as calculating distances to the nearest hospital.\n"
      ],
      "id": "32e87eb6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import geopandas as gpd\n",
        "\n",
        "# Calculate centroids for each ZIP code and create a new GeoDataFrame with these centroids\n",
        "zip_shapefile['centroid'] = zip_shapefile.geometry.centroid  # Calculate centroids\n",
        "\n",
        "# Convert this information into a new GeoDataFrame, focusing only on the centroids and ZIP code info\n",
        "zips_all_centroids = gpd.GeoDataFrame(zip_shapefile[['ZCTA5', 'centroid']], geometry='centroid')\n",
        "\n",
        "# Rename columns for clarity\n",
        "zips_all_centroids.columns = ['ZIP Code', 'geometry']\n",
        "\n",
        "# Display the dimensions and column descriptions\n",
        "print(\"Dimensions of zips_all_centroids:\", zips_all_centroids.shape)\n",
        "print(\"Columns in zips_all_centroids:\", zips_all_centroids.columns)\n",
        "\n",
        "# Display the first few rows to verify the output\n",
        "zips_all_centroids.head()"
      ],
      "id": "d8b6c6d8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. \n"
      ],
      "id": "423a444c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Define ZIP code prefixes for Texas and its neighboring states\n",
        "texas_prefixes = list(range(750, 799))\n",
        "# Adding neighboring state prefixes\n",
        "bordering_prefixes = texas_prefixes + [70, 71, 72, 73, 74, 87, 88]\n",
        "\n",
        "# Convert prefixes to strings for matching with the 'ZIP Code' column\n",
        "texas_prefixes_str = tuple(map(str, texas_prefixes))\n",
        "bordering_prefixes_str = tuple(map(str, bordering_prefixes))\n",
        "\n",
        "# Filter the zips_all_centroids GeoDataFrame for Texas ZIP codes\n",
        "zips_texas_centroids = zips_all_centroids[\n",
        "    zips_all_centroids['ZIP Code'].str.startswith(texas_prefixes_str)\n",
        "]\n",
        "\n",
        "# Filter the zips_all_centroids GeoDataFrame for Texas and bordering states ZIP codes\n",
        "zips_texas_borderstates_centroids = zips_all_centroids[\n",
        "    zips_all_centroids['ZIP Code'].str.startswith(bordering_prefixes_str)\n",
        "]\n",
        "\n",
        "# Print out the number of unique ZIP codes in each subset\n",
        "num_texas_zips = zips_texas_centroids['ZIP Code'].nunique()\n",
        "num_bordering_zips = zips_texas_borderstates_centroids['ZIP Code'].nunique()\n",
        "\n",
        "print(\"Unique ZIP codes in Texas subset:\", num_texas_zips)\n",
        "print(\"Unique ZIP codes in Texas and bordering states subset:\", num_bordering_zips)\n"
      ],
      "id": "c8a2f3f0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3. \n",
        "\n",
        "We used an inner join to retain only the ZIP codes in Texas and neighboring states that have at least one hospital, ensuring that `zips_withhospital_centroids` includes only relevant ZIP codes. The join was conducted on the `ZIP Code` column from `zips_texas_borderstates_centroids` and `ZIP_CD` from `zips_with_hospital`, allowing us to filter specifically for ZIP codes that had at least one active hospital in 2016. The result is a GeoDataFrame, `zips_withhospital_centroids`, which contains only the ZIP codes in Texas or bordering states with at least one hospital in that year.\n"
      ],
      "id": "9923f279"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Step 1: Filter hospitals active in 2016 and count by ZIP code\n",
        "\n",
        "hospitals_per_zip = combined_df[(combined_df['Year'] == 2016) & (\n",
        "    combined_df['PGM_TRMNTN_CD'] == \"00\")].groupby('ZIP_CD').size().reset_index(name='hospital_count')\n",
        "\n",
        "\n",
        "# Filter to keep only ZIP codes with at least one hospital\n",
        "zips_with_hospital = hospitals_per_zip[hospitals_per_zip['hospital_count'] > 0]\n",
        "\n",
        "# Step 2: Merge this count data with zips_texas_borderstates_centroids on ZIP code\n",
        "# Ensuring ZIP codes match data types between datasets\n",
        "zips_texas_borderstates_centroids['ZIP Code'] = zips_texas_borderstates_centroids['ZIP Code'].astype(\n",
        "    str)\n",
        "zips_with_hospital['ZIP_CD'] = zips_with_hospital['ZIP_CD'].astype(str)\n",
        "\n",
        "# Perform an inner join to keep only ZIP codes with hospitals in 2016\n",
        "zips_withhospital_centroids = zips_texas_borderstates_centroids.merge(\n",
        "    zips_with_hospital, left_on='ZIP Code', right_on='ZIP_CD', how='inner'\n",
        ")\n",
        "\n",
        "# Display\n",
        "display(zips_withhospital_centroids.head())"
      ],
      "id": "2a0941c7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4. \n",
        "    a.\n"
      ],
      "id": "768ab3b2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from shapely.ops import nearest_points\n",
        "import time\n",
        "\n",
        "# The geometry column is set as active in both GeoDataFrames\n",
        "zips_texas_centroids = zips_texas_centroids.set_geometry('geometry')\n",
        "zips_withhospital_centroids = zips_withhospital_centroids.set_geometry('geometry')\n",
        "\n",
        "# Subset to 10 ZIP codes from zips_texas_centroids for testing\n",
        "test_zips_texas = zips_texas_centroids.head(10)\n",
        "\n",
        "# Start timer\n",
        "start_time = time.time()\n",
        "\n",
        "# Calculate distances for each ZIP code in test subset\n",
        "distances = []\n",
        "for idx, row in test_zips_texas.iterrows():\n",
        "    # Calculate the distance to the nearest hospital ZIP code\n",
        "    nearest_hospital = zips_withhospital_centroids.distance(row.geometry).min()\n",
        "    distances.append(nearest_hospital)\n",
        "\n",
        "# End timer\n",
        "end_time = time.time()\n",
        "elapsed_time = end_time - start_time\n",
        "\n",
        "# Output the time taken and estimated total time for the full dataset\n",
        "print(\"Time taken for 10 ZIP codes:\", elapsed_time, \"seconds\")\n",
        "\n",
        "# Estimate total time \n",
        "total_rows = len(zips_texas_centroids)\n",
        "estimated_total_time = (elapsed_time / 10) * total_rows\n",
        "print(\"Estimated time for entire dataset:\", estimated_total_time, \"seconds\")"
      ],
      "id": "634684fc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "    b.\n",
        "The full dataset calculation completed significantly faster than initially estimated, suggesting that the small sample used for estimation might have over-predicted the time required. This difference could be due to optimizations in the spatial library that enhance performance with larger datasets, as well as variations in system performance between runs. Small sample estimates can sometimes lead to overestimations when the operation scales more efficiently across a larger volume of data. \n"
      ],
      "id": "fcde6a51"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import time\n",
        "\n",
        "# Start timer for the full calculation\n",
        "start_time_full = time.time()\n",
        "\n",
        "# Calculate distances for each ZIP code in the entire Texas subset\n",
        "full_distances = []\n",
        "for idx, row in zips_texas_centroids.iterrows():\n",
        "    # Calculate the distance to the nearest hospital ZIP code\n",
        "    nearest_hospital_distance = zips_withhospital_centroids.distance(row.geometry).min()\n",
        "    full_distances.append(nearest_hospital_distance)\n",
        "\n",
        "# End timer\n",
        "end_time_full = time.time()\n",
        "elapsed_time_full = end_time_full - start_time_full\n",
        "\n",
        "# Output the actual time taken\n",
        "print(\"Actual time taken for full dataset:\", elapsed_time_full, \"seconds\")\n",
        "\n",
        "# Compare to estimated time from previous step\n",
        "estimated_total_time = (elapsed_time / 10) * len(zips_texas_centroids)  # Using the previous subset estimate\n",
        "print(\"Estimated time for entire dataset:\", estimated_total_time, \"seconds\")\n",
        "print(\"Difference between actual and estimated time:\", abs(elapsed_time_full - estimated_total_time), \"seconds\")\n"
      ],
      "id": "646c5b7c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "    c.\n",
        "\n",
        "In the `.prj` file , the unit is degrees. This indicates that the geographic data is in a coordinate system based on latitude and longitude, with units in angular degrees rather than linear distance. \n",
        "\n",
        "To convert degrees to miles, we can use an approximate conversion. Each degree of latitude corresponds to about 69 miles, a consistent value because latitude lines are parallel. For longitude, the conversion varies depending on latitude due to the Earth’s curvature; at the equator, 1 degree of longitude equals approximately 69 miles, but this distance decreases as you move toward the poles. A common formula to estimate the distance per degree of longitude at a given latitude L  is  69 x cos(L) . \n",
        "\n",
        "5. \n",
        "    a.\n",
        "\n",
        " The distances are converted from degrees to miles.\n"
      ],
      "id": "ce6ac0ed"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import geopandas as gpd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Calculate the distance to the nearest hospital for each ZIP code in Texas\n",
        "zips_texas_centroids['nearest_hospital_distance'] = zips_texas_centroids.geometry.apply(\n",
        "    lambda x: zips_withhospital_centroids.distance(x).min()\n",
        ")\n",
        "\n",
        "# Step 2: Calculate the average distance (in degrees)\n",
        "avg_dist_degrees = zips_texas_centroids['nearest_hospital_distance'].mean()\n",
        "\n",
        "# Step 3: Convert average distance from degrees to miles\n",
        "avg_dist_miles = avg_dist_degrees * 69  # 1 degree ≈ 69 miles\n",
        "\n",
        "print(\"Average distance to the nearest hospital (in degrees):\", avg_dist_degrees)\n",
        "print(\"Average distance to the nearest hospital (in miles):\", avg_dist_miles)"
      ],
      "id": "407c7c22",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "   b.\n"
      ],
      "id": "b8cc202d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Step 4: Map the value for each ZIP code in Texas\n",
        "# Convert each ZIP code's distance to miles for mapping\n",
        "zips_texas_centroids['nearest_hospital_distance_miles'] = zips_texas_centroids['nearest_hospital_distance'] * 69\n",
        "\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
        "zips_texas_centroids.plot(column='nearest_hospital_distance_miles', cmap='coolwarm', legend=True, ax=ax)\n",
        "ax.set_title(\"Distance to Nearest Hospital for Each ZIP Code in Texas (Miles)\")\n",
        "ax.set_xlabel(\"Longitude\")\n",
        "ax.set_ylabel(\"Latitude\")\n",
        "plt.show()"
      ],
      "id": "d6f120e0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The choropleth map visually represents the distance to the nearest hospital for each ZIP code in Texas, showing regions with greater or lesser hospital access.\n",
        "\n",
        "## Effects of closures on access in Texas (15 pts)\n",
        "\n",
        "\n",
        "1. \n"
      ],
      "id": "ab99c5ef"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Filter for Texas ZIP codes in the range 75000 to 79999 and for closures between 2016 and 2019\n",
        "texas_closures = corrected_closures[\n",
        "    (corrected_closures['ZIP Code'].astype(str).str.startswith(tuple(map(str, range(750, 799))))) & \n",
        "    (corrected_closures['Year Closed'].between(\"2016\", \"2019\"))\n",
        "]\n",
        "\n",
        "# Group by ZIP Code and count the number of closures for each ZIP code\n",
        "closures_by_zipcode = texas_closures.groupby('ZIP Code').size().reset_index(name='Number of Closures')\n",
        "\n",
        "# Display the result\n",
        "closures_by_zipcode"
      ],
      "id": "47657f42",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. \n"
      ],
      "id": "c06afd69"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import geopandas as gpd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the ZIP code shapefile\n",
        "zip_shapefile = gpd.read_file('gz_2010_us_860_00_500k/gz_2010_us_860_00_500k.shp')\n",
        "\n",
        "# Filter for Texas ZIP codes (ZIP codes starting with 750-799)\n",
        "zip_shapefile = zip_shapefile[zip_shapefile['ZCTA5'].str.startswith(tuple(map(str, range(750, 800))))]\n",
        "\n",
        "# Ensure ZIP code columns are strings for matching\n",
        "zip_shapefile['ZCTA5'] = zip_shapefile['ZCTA5'].astype(str)\n",
        "closures_by_zipcode['ZIP Code'] = closures_by_zipcode['ZIP Code'].astype(str)\n",
        "\n",
        "# Merge the shapefile with closures data on ZIP code\n",
        "texas_zip_closures = zip_shapefile.merge(closures_by_zipcode, left_on='ZCTA5', right_on='ZIP Code', how='left')\n",
        "\n",
        "# Replace NaN values in 'Number of Closures' with 0 for ZIP codes without closures\n",
        "texas_zip_closures['Number of Closures'] = texas_zip_closures['Number of Closures'].fillna(0)\n",
        "\n",
        "# Count the number of directly affected ZIP codes (where Number of Closures > 0)\n",
        "affected_zip_count = texas_zip_closures[texas_zip_closures['Number of Closures'] > 0].shape[0]\n",
        "print(f\"Number of directly affected ZIP codes in Texas: {affected_zip_count}\")\n",
        "\n",
        "# Plot the choropleth map\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
        "texas_zip_closures.plot(column='Number of Closures', cmap='YlGnBu', linewidth=0.8, ax=ax, edgecolor='0.8', legend=True)\n",
        "ax.set_title('Texas ZIP Codes Directly Affected by Hospital Closures (2016-2019)')\n",
        "ax.set_axis_off()\n",
        "plt.show()"
      ],
      "id": "097e25e2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3. \n",
        "4. \n",
        "\n",
        "## Reflecting on the exercise (10 pts) "
      ],
      "id": "1b3fa63e"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "C:\\Users\\aosil\\AppData\\Local\\Programs\\Python\\Python312\\share\\jupyter\\kernels\\python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}